{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd8bd3e-ec28-4f49-9cc0-af987bc96e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from speech_reader import speech_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba95d87c-b577-42d0-b00c-6ddfd47d7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel,AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbdc56af-ca87-4ea0-a672-3adf4ccc5813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "custom_token = \"<sep>\"\n",
    "tokenizer.add_tokens([custom_token])\n",
    "\n",
    "# Load the model\n",
    "folder_path = \"../models/\"\n",
    "# checkpoint = f\"checkpoint-{240}\"\n",
    "model_name = \"model_3epoch\"\n",
    "model_path = folder_path + model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617322c-7df1-47cc-b169-b0cd8334af30",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bbe9f6b-259b-4808-98a5-6c7fcc4d2e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech_id</th>\n",
       "      <th>speech</th>\n",
       "      <th>date</th>\n",
       "      <th>char_count</th>\n",
       "      <th>speaker</th>\n",
       "      <th>word_count</th>\n",
       "      <th>speakerid</th>\n",
       "      <th>lastname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>district</th>\n",
       "      <th>nonvoting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1140000007</td>\n",
       "      <td>RODGERS. Madam Clerk. it is an honor to addres...</td>\n",
       "      <td>20150106</td>\n",
       "      <td>153</td>\n",
       "      <td>Mrs. McMORRIS</td>\n",
       "      <td>267</td>\n",
       "      <td>114120480</td>\n",
       "      <td>MCMORRIS RODGERS</td>\n",
       "      <td>CATHY</td>\n",
       "      <td>H</td>\n",
       "      <td>WA</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>5.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1140000009</td>\n",
       "      <td>Madam Clerk. first I would like to recognize e...</td>\n",
       "      <td>20150106</td>\n",
       "      <td>135</td>\n",
       "      <td>Mr. BECERRA</td>\n",
       "      <td>244</td>\n",
       "      <td>114118560</td>\n",
       "      <td>BECERRA</td>\n",
       "      <td>XAVIER</td>\n",
       "      <td>H</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>34.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1140000011</td>\n",
       "      <td>Madam Clerk. I present for election to the off...</td>\n",
       "      <td>20150106</td>\n",
       "      <td>24</td>\n",
       "      <td>Mr. MASSIE</td>\n",
       "      <td>41</td>\n",
       "      <td>114121890</td>\n",
       "      <td>MASSIE</td>\n",
       "      <td>THOMAS</td>\n",
       "      <td>H</td>\n",
       "      <td>KY</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>4.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1140000013</td>\n",
       "      <td>Madam Clerk. I present for the election of the...</td>\n",
       "      <td>20150106</td>\n",
       "      <td>101</td>\n",
       "      <td>Mr. BRIDENSTINE</td>\n",
       "      <td>183</td>\n",
       "      <td>114122500</td>\n",
       "      <td>BRIDENSTINE</td>\n",
       "      <td>JIM</td>\n",
       "      <td>H</td>\n",
       "      <td>OK</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1140000015</td>\n",
       "      <td>Madam Clerk. I rise to place in a nomination f...</td>\n",
       "      <td>20150106</td>\n",
       "      <td>55</td>\n",
       "      <td>Mr. KING of Iowa</td>\n",
       "      <td>92</td>\n",
       "      <td>114120060</td>\n",
       "      <td>KING</td>\n",
       "      <td>STEVE</td>\n",
       "      <td>H</td>\n",
       "      <td>IA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>4.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    speech_id                                             speech      date  \\\n",
       "0  1140000007  RODGERS. Madam Clerk. it is an honor to addres...  20150106   \n",
       "1  1140000009  Madam Clerk. first I would like to recognize e...  20150106   \n",
       "2  1140000011  Madam Clerk. I present for election to the off...  20150106   \n",
       "3  1140000013  Madam Clerk. I present for the election of the...  20150106   \n",
       "4  1140000015  Madam Clerk. I rise to place in a nomination f...  20150106   \n",
       "\n",
       "   char_count           speaker  word_count  speakerid          lastname  \\\n",
       "0         153     Mrs. McMORRIS         267  114120480  MCMORRIS RODGERS   \n",
       "1         135       Mr. BECERRA         244  114118560           BECERRA   \n",
       "2          24        Mr. MASSIE          41  114121890            MASSIE   \n",
       "3         101   Mr. BRIDENSTINE         183  114122500       BRIDENSTINE   \n",
       "4          55  Mr. KING of Iowa          92  114120060              KING   \n",
       "\n",
       "  firstname chamber state gender party  district nonvoting  \n",
       "0     CATHY       H    WA      F     R       5.0    voting  \n",
       "1    XAVIER       H    CA      M     D      34.0    voting  \n",
       "2    THOMAS       H    KY      M     R       4.0    voting  \n",
       "3       JIM       H    OK      M     R       1.0    voting  \n",
       "4     STEVE       H    IA      M     R       4.0    voting  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches = speech_reader(year = '114')\n",
    "speeches_df = speeches.dataset\n",
    "speeches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48a2068-8503-4d3c-a1c8-45ceb6579d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/nzvws_f9091g0pl89431q7zw0000gn/T/ipykernel_27676/3203135855.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  senate_speeches['speech'] = senate_speeches.speech.str.replace(\"\\n\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senate_speeches = speeches_df[speeches_df.chamber == 'S']\n",
    "senate_speeches['speech'] = senate_speeches.speech.str.replace(\"\\n\", \" \")\n",
    "senate_speeches.date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67feaeae-5d83-400a-9cfa-95669df68047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jan_speeches = senate_speeches[senate_speeches['date'].str.startswith('201501')]\n",
    "# feb_speeches = senate_speeches[senate_speeches['date'].str.startswith('201502')]\n",
    "# march_speeches = senate_speeches[senate_speeches['date'].str.startswith('201503')]\n",
    "# april_speeches = senate_speeches[senate_speeches['date'].str.startswith('201503')]\n",
    "# march_speeches = senate_speeches[senate_speeches['date'].str.startswith('201503')]\n",
    "speeches_list = []\n",
    "months = ['201501', '201502', '201503', '201504', '201505']\n",
    "for month in months:\n",
    "    speeches_list.append(senate_speeches[senate_speeches['date'].str.startswith(month)])\n",
    "    \n",
    "train_speeches = pd.concat(speeches_list)\n",
    "train_speeches = train_speeches[train_speeches.word_count>70]\n",
    "# train_speeches = train_speeches.sample(100)\n",
    "train_speeches.reset_index(drop= True, inplace=True)\n",
    "len(train_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac6e1ed5-ef0f-4c8b-89bc-f1d290def3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mr. MCCONNELL', 'Mr. DURBIN', 'Mr. McCONNELL', 'Mr. HATCH',\n",
       "       'Mr. LEAHY', 'Mrs. FEINSTEIN', 'Ms. COLLINS', 'Mr. REED',\n",
       "       'Mr. UDALL', 'Mr. MERKLEY', 'Mr. BLUNT', 'Mr. THUNE',\n",
       "       'Mr. SANDERS', 'Mr. CORNYN', 'Mr. WHITEHOUSE', 'Mr. HOEVEN',\n",
       "       'Mr. MANCHIN', 'Mr. DAINES', 'Mr. BARRASSO', 'Ms. MURKOWSKI',\n",
       "       'Ms. HEITKAMP', 'Mr. ROBERTS', 'Mr. CARDIN', 'Mr. MARKEY',\n",
       "       'Mr. BROWN', 'Mr. WYDEN', 'Mr. CASEY', 'Mr. SCOTT',\n",
       "       'Mrs. GILLIBRAND', 'Mr. ALEXANDER', 'Mr. BENNET', 'Mr. BOOKER',\n",
       "       'Mr. BURR', 'Mr. KING', 'Mr. CRAPO', 'Mr. SCHUMER', 'Mr. COATS',\n",
       "       'Mr. HELLER', 'Ms. STABENOW', 'Mrs. FISCHER', 'Ms. WARREN',\n",
       "       'Mr. INHOFE', 'Mr. COONS', 'Mr. WICKER', 'Mr. COCHRAN',\n",
       "       'Mr. BLUMENTHAL', 'Mr. NELSON', 'Ms. CANTWELL', 'Mr. VITTER',\n",
       "       'Mrs. SHAHEEN', 'Mr. SCHATZ', 'Mr. SESSIONS', 'Mr. KAINE',\n",
       "       'Mr. TOOMEY', 'Mrs. McCASKILL', 'Mr. WARNER', 'Mrs. BOXER',\n",
       "       'Mr. BOOZMAN', 'Mr. MENENDEZ', 'Mrs. MURRAY', 'Mr. PORTMAN',\n",
       "       'Mr. CASSIDY', 'Mr. GRASSLEY', 'Mr. CARPER', 'Ms. AYOTTE',\n",
       "       'Mr. ISAKSON', 'Ms. HIRONO', 'Mrs. MCCASKILL', 'Mr. FRANKEN',\n",
       "       'Mr. MURPHY', 'Ms. MIKULSKI', 'Mr. REID', 'Mr. CORKER',\n",
       "       'Mrs. CAPITO', 'Mr. SHELBY', 'Ms. KLOBUCHAR', 'Mr. LEE',\n",
       "       'Mr. GRAHAM', 'Mr. TILLIS', 'Mr. MCCAIN', 'Mr. MORAN', 'Mr. ENZI',\n",
       "       'Mr. HEINRICH', 'Mr. TESTER', 'Mr. FLAKE', 'Mr. JOHNSON',\n",
       "       'Mr. PETERS', 'Mr. SULLIVAN', 'Mr. CRUZ', 'Ms. FEINSTEIN',\n",
       "       'Mr. PERDUE', 'Ms. BALDWIN', 'Mr. RISCH', 'Ms. HEITIKAMP',\n",
       "       'MR. HATCH', 'Mr. RUBIO', 'Mr. KIRK', 'MER. DURBIN',\n",
       "       'MER. GILLIBRAND', 'Mr. DONNELLY', 'Ms. JACKSON LEE', 'Mr. COTTON',\n",
       "       'Mrs. ERNST', 'Mr. ROUNDS', 'Mr. GARDNER', 'Mr. PAUL',\n",
       "       'Ms. JASMINE MURRAY', 'Ms. MURRAY', 'Mr. LANKFORD',\n",
       "       'Mrs. STABENOW'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_speeches.speaker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e885c415-3da7-46d1-8c85-e2d00d487d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset  = train_speeches[['speaker', 'speech']].agg(': <sep> '.join, axis=1).to_list()\n",
    "# dataset = random.sample(dataset, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26830741-b4d3-4470-aa2e-f8d0bb98855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanpunctuation(s):\n",
    "    for p in '!,.:;?':\n",
    "        s=s.replace(' '+p,p)\n",
    "    s=s.replace(' '+'n\\'t','n\\'t')\n",
    "    s=s.replace(' '+'\\'s','\\'s')\n",
    "    s=s.replace(' '+'\\'re','\\'re')\n",
    "    s=s.replace(' '+'\\'ve','\\'ve')\n",
    "    s=s.replace(' '+'\\'ll','\\'ll')\n",
    "    s=s.replace(' '+'\\'am','\\'am')\n",
    "    s=s.replace(' '+'\\'m','\\'m')\n",
    "    s=s.replace(' '+'\\' m','\\'m')\n",
    "    s=s.replace(' '+'\\'m','\\'m')\n",
    "    s=s.replace(' '+'\\' ve','\\'ve')\n",
    "    s=s.replace(' '+'\\' s','\\'s')\n",
    "    s=s.replace('<newline>','\\n')\n",
    "    s=s.replace('-','')\n",
    "    s=s.replace('\\xa0',' ')\n",
    "    \n",
    "    return s   \n",
    "\n",
    "text_dataset=list(map(cleanpunctuation,dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11c90f81-d880-4e0b-b6df-4b672b4c88c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. DURBIN: <sep> Mr. President. I thank the majority leader for those kind words. I am happy to report the Democratic leader of the Senate. Senator REID. is making a speedy recovery from his New Years runin with some exercise equipment. His face and ribs are still sore. He is eager to get back to work. We met with him this morning. and we can expect him back in the Senate very soon. In the meantime. it is a privilege on behalf of the Democratic Caucus to welcome our old colleagues back to work and welcome our new colleagues and their families to the U.S. Senate. I also want to wish Leader MCCONNELL. as he takes up the new duties of the majority leader. the very best. Senator Dirksen was a Senator from my home State of Illinois who served as a Republican leader of the Senate from 1959 to 1969. He famously said. \"I am a man of fixed and unbending principles. the first of which is to be flexible at all times.\" That may sound comical. even contradictory. But Senator Dirksens ability on flexible tactics and firmness on principles helped produce historic legislation such as the Civil Rights Act of 1964. one of the greatest achievements in our Nations history. I am sure we all will remember that with fondness and pride. The American people need us to work together to solve problems and create opportunities. For their sake. let us all try to remember that what we are about is honorable compromise. The Constitution of the United States and the Senate itself are the results of just such a compromise. One other point. One hundred years ago this week. an American industrialist and entrepreneur stunned the world by announcing he would start paying his workers double the industry average and cut the hours. That man. of course. was Henry Ford. He committed to pay his workers a minimum wage. As we begin this new Congress. let us dedicate ourselves to the working men and women across America. the taxpayers of this country. and the men and women which we so proudly serve. I hope that we will show flexibility and principle. We cant solve Americas challenges with the same old thinking. We have to address the problems with mutual respect and with a positive attitude. I look forward to. on this side of the aisle. working with Senator REID and my colleagues to achieve that end. Congratulations to Leader MCCONNELL. '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36666c-e4bf-4d1f-a96f-d417d9300610",
   "metadata": {},
   "source": [
    "### tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23eb3388-6d8d-4808-af65-2ca02408b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "encoded_data = tokenizer(text_dataset, padding=True,truncation=True,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab8224c4-468f-473d-9537-570c74033bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(inputs):\n",
    "    labels=[]\n",
    "    for ids,attention_mask in zip(inputs['input_ids'],inputs['attention_mask']):\n",
    "        label=ids.copy()\n",
    "        real_len=sum(attention_mask)\n",
    "        padding_len=len(attention_mask)-sum(attention_mask)\n",
    "        label[:]=label[:real_len]+[-100]*padding_len\n",
    "        labels.append(label)\n",
    "    inputs['labels']=labels\n",
    "    \n",
    "create_labels(encoded_data)\n",
    "encoded_data = Dataset.from_dict(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85bbb1f0-1a45-4857-aa08-a728ee67b01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfffa2ef65904095a943e17dbe40c762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47da7ce26e71474b93fbf6335636f0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fe65c4f98e43f385d7add779b9d943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/309 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 90% train, 10% test + validation\n",
    "train_testvalid = encoded_data.train_test_split(test_size=0.2)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "encoded_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "encoded_dataset.save_to_disk(\"../data/encoded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d287d-9df2-4575-891b-88eb0b669671",
   "metadata": {},
   "source": [
    "## load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f8ed43-acf3-4454-a93d-86519b533dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset.save_to_disk(\"../data/encoded\")\n",
    "# ...\n",
    "from datasets import load_from_disk\n",
    "encoded_dataset = load_from_disk(\"../data/encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1d2b91-a85f-4b78-996c-9957c33f2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subset the datasets\n",
    "train_subset = encoded_dataset[\"train\"].select(range(1000))\n",
    "test_subset = encoded_dataset[\"test\"].select(range(100))\n",
    "valid_subset = encoded_dataset[\"valid\"].select(range(100))\n",
    "\n",
    "# Create a new DatasetDict with your subsets\n",
    "encoded_dataset = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"test\": test_subset,\n",
    "    \"valid\": valid_subset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d31395-4976-4678-9040-3c20303a4b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7afd63e1-bb21-40d5-8b09-06fc68108059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset:\n",
    "    def __init__(self, inputs):\n",
    "        self.ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.labels=inputs['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        return [torch.tensor(self.ids[item], dtype=torch.long),\n",
    "                torch.tensor(self.attention_mask[item], dtype=torch.long),\n",
    "                torch.tensor(self.labels[item], dtype=torch.long)]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8de614f-cc6b-41ae-91bd-bed97d9892c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size= 8\n",
    "valid_batch_size= 16\n",
    "traindata=StoryDataset(encoded_dataset['train'])\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    traindata,\n",
    "    shuffle=False,\n",
    "    batch_size=train_batch_size)\n",
    "\n",
    "validdata=StoryDataset(encoded_dataset['valid'])\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    validdata,\n",
    "    shuffle=False,\n",
    "    batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba554ae7-db29-4e68-86d3-c80129f49218",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "360d93dc-7b7e-465a-a146-4f4570349a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics (assuming accuracy for classification)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88f5775f-36ea-47d8-8c21-5800023d4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List all checkpoints\n",
    "model_path = \"../models/\"\n",
    "checkpoints = [os.path.join(model_path, d) for d in os.listdir(model_path) if d.startswith(\"checkpoint\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fad8439e-5b88-481e-b1fe-fb4b65392f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ../models/checkpoint-420...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m checkpoint \u001b[38;5;129;01min\u001b[39;00m checkpoints:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/DialoGPT-medium\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize variables to track the best model\n",
    "best_checkpoint = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Evaluate each checkpoint\n",
    "for checkpoint in checkpoints:\n",
    "    print(f\"Evaluating {checkpoint}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "    \n",
    "    # tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
    "    # Define training arguments (adjust as needed)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_path,\n",
    "        per_device_eval_batch_size=32,\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance for evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset['train'],\n",
    "        eval_dataset=encoded_dataset['validation'],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Acc: {eval_result['eval_accuracy']}\")\n",
    "\n",
    "    # Check if this is the best model so far\n",
    "    if eval_result[\"eval_accuracy\"] > best_accuracy:\n",
    "        best_accuracy = eval_result[\"eval_accuracy\"]\n",
    "        best_checkpoint = checkpoint\n",
    "\n",
    "# Output the best model\n",
    "print(f\"Best model checkpoint: {best_checkpoint} with accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd17c0-98c7-4d4c-b6a1-ba275e578687",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219f722-5056-45c4-90ff-5f33e8a3f12f",
   "metadata": {},
   "source": [
    "### method1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed0742f-a6ef-4606-94a5-f178054e5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models\", #The output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    # load_best_model_at_end = True, \n",
    "    eval_steps = 40, # Number of update steps between two evaluations.\n",
    "    save_steps=50, # after # steps model is saved \n",
    "    warmup_steps=50# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['valid']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78062e3d-ebf9-4d33-8400-9becd0e2f23f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|                                               | 0/7 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m d1,d2,d3\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     batch_loss\u001b[38;5;241m=\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m eval_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m[batch_loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 354\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/pytorch_utils.py:106\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    105\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.to('cuda')\n",
    "model.eval()\n",
    "eval_loss=[]\n",
    "for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
    "    d1,d2,d3=inputs\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
    "        batch_loss=output[0]\n",
    "    eval_loss+=[batch_loss.cpu().item()]\n",
    "    del batch_loss\n",
    "eval_loss=np.mean(eval_loss)\n",
    "perplexity=math.exp(eval_loss)\n",
    "print(f'The average perplexity for valid dataset before fine-tuning is {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e17f9-d631-4aea-ae05-a2e0988bad2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='202' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [202/250 4:52:07 < 1:10:06, 0.01 it/s, Epoch 1.61/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07f40b86-5c8f-49ee-959f-e78cff1b3103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|██████████████████████████████████████| 7/7 [11:42<00:00, 100.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average perplexity for valid dataset after fine-tuning is 16.192074924368082\n"
     ]
    }
   ],
   "source": [
    "# model.to('cuda')\n",
    "model.eval()\n",
    "eval_loss=[] \n",
    "for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
    "    d1,d2,d3=inputs\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
    "        batch_loss=output[0]\n",
    "    eval_loss+=[batch_loss.cpu().item()]\n",
    "    del batch_loss\n",
    "eval_loss=np.mean(eval_loss)\n",
    "perplexity=math.exp(eval_loss)\n",
    "print(f'The average perplexity for valid dataset after fine-tuning is {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d109629e-bad0-4098-baa9-f6697b4f5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/1000_retoken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
