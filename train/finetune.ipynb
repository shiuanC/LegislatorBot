{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8bd3e-ec28-4f49-9cc0-af987bc96e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from speech_reader import speech_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95d87c-b577-42d0-b00c-6ddfd47d7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel,AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc56af-ca87-4ea0-a672-3adf4ccc5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "custom_token = \"<sep>\" #add the special token to split the speaker and the speach\n",
    "tokenizer.add_tokens([custom_token])\n",
    "\n",
    "#start from gpt\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the model\n",
    "folder_path = \"../models/\"\n",
    "# checkpoint = f\"checkpoint-{240}\"\n",
    "# continued based on previous checkpoint\n",
    "model_name = \"model_3epoch\"\n",
    "model_path = folder_path + model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617322c-7df1-47cc-b169-b0cd8334af30",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe9f6b-259b-4808-98a5-6c7fcc4d2e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speeches = speech_reader(year = '114')\n",
    "speeches_df = speeches.dataset\n",
    "speeches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a2068-8503-4d3c-a1c8-45ceb6579d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_speeches = speeches_df[speeches_df.chamber == 'S']\n",
    "senate_speeches['speech'] = senate_speeches.speech.str.replace(\"\\n\", \" \")\n",
    "senate_speeches.date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67feaeae-5d83-400a-9cfa-95669df68047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jan_speeches = senate_speeches[senate_speeches['date'].str.startswith('201501')]\n",
    "# feb_speeches = senate_speeches[senate_speeches['date'].str.startswith('201502')]\n",
    "# march_speeches = senate_speeches[senate_speeches['date'].str.startswith('201503')]\n",
    "# april_speeches = senate_speeches[senate_speeches['date'].str.startswith('201503')]\n",
    "# march_speeches = senate_speeches[senate_speeches['date'].str.startswith('201503')]\n",
    "speeches_list = []\n",
    "months = ['201501', '201502', '201503', '201504', '201505']\n",
    "for month in months:\n",
    "    speeches_list.append(senate_speeches[senate_speeches['date'].str.startswith(month)])\n",
    "    \n",
    "train_speeches = pd.concat(speeches_list)\n",
    "train_speeches = train_speeches[train_speeches.word_count>70]\n",
    "# train_speeches = train_speeches.sample(100)\n",
    "train_speeches.reset_index(drop= True, inplace=True)\n",
    "len(train_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e1ed5-ef0f-4c8b-89bc-f1d290def3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speeches.speaker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885c415-3da7-46d1-8c85-e2d00d487d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset  = train_speeches[['speaker', 'speech']].agg(': <sep> '.join, axis=1).to_list()\n",
    "# dataset = random.sample(dataset, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26830741-b4d3-4470-aa2e-f8d0bb98855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanpunctuation(s):\n",
    "    for p in '!,.:;?':\n",
    "        s=s.replace(' '+p,p)\n",
    "    s=s.replace(' '+'n\\'t','n\\'t')\n",
    "    s=s.replace(' '+'\\'s','\\'s')\n",
    "    s=s.replace(' '+'\\'re','\\'re')\n",
    "    s=s.replace(' '+'\\'ve','\\'ve')\n",
    "    s=s.replace(' '+'\\'ll','\\'ll')\n",
    "    s=s.replace(' '+'\\'am','\\'am')\n",
    "    s=s.replace(' '+'\\'m','\\'m')\n",
    "    s=s.replace(' '+'\\' m','\\'m')\n",
    "    s=s.replace(' '+'\\'m','\\'m')\n",
    "    s=s.replace(' '+'\\' ve','\\'ve')\n",
    "    s=s.replace(' '+'\\' s','\\'s')\n",
    "    s=s.replace('<newline>','\\n')\n",
    "    s=s.replace('-','')\n",
    "    s=s.replace('\\xa0',' ')\n",
    "    \n",
    "    return s   \n",
    "\n",
    "text_dataset=list(map(cleanpunctuation,dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c90f81-d880-4e0b-b6df-4b672b4c88c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36666c-e4bf-4d1f-a96f-d417d9300610",
   "metadata": {},
   "source": [
    "### tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb3388-6d8d-4808-af65-2ca02408b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "encoded_data = tokenizer(text_dataset, padding=True,truncation=True,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8224c4-468f-473d-9537-570c74033bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(inputs):\n",
    "    labels=[]\n",
    "    for ids,attention_mask in zip(inputs['input_ids'],inputs['attention_mask']):\n",
    "        label=ids.copy()\n",
    "        real_len=sum(attention_mask)\n",
    "        padding_len=len(attention_mask)-sum(attention_mask)\n",
    "        label[:]=label[:real_len]+[-100]*padding_len\n",
    "        labels.append(label)\n",
    "    inputs['labels']=labels\n",
    "    \n",
    "create_labels(encoded_data)\n",
    "encoded_data = Dataset.from_dict(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbb1f0-1a45-4857-aa08-a728ee67b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% train, 10% test + validation\n",
    "train_testvalid = encoded_data.train_test_split(test_size=0.2)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "encoded_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "encoded_dataset.save_to_disk(\"../data/encoded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d287d-9df2-4575-891b-88eb0b669671",
   "metadata": {},
   "source": [
    "## load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8ed43-acf3-4454-a93d-86519b533dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset.save_to_disk(\"../data/encoded\")\n",
    "# ...\n",
    "from datasets import load_from_disk\n",
    "encoded_dataset = load_from_disk(\"../data/encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d2b91-a85f-4b78-996c-9957c33f2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subset the datasets\n",
    "train_subset = encoded_dataset[\"train\"].select(range(1000))\n",
    "test_subset = encoded_dataset[\"test\"].select(range(100))\n",
    "valid_subset = encoded_dataset[\"valid\"].select(range(100))\n",
    "\n",
    "# Create a new DatasetDict with your subsets\n",
    "encoded_dataset = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"test\": test_subset,\n",
    "    \"valid\": valid_subset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d31395-4976-4678-9040-3c20303a4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd63e1-bb21-40d5-8b09-06fc68108059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset:\n",
    "    def __init__(self, inputs):\n",
    "        self.ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.labels=inputs['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        return [torch.tensor(self.ids[item], dtype=torch.long),\n",
    "                torch.tensor(self.attention_mask[item], dtype=torch.long),\n",
    "                torch.tensor(self.labels[item], dtype=torch.long)]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de614f-cc6b-41ae-91bd-bed97d9892c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size= 8\n",
    "valid_batch_size= 16\n",
    "traindata=StoryDataset(encoded_dataset['train'])\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    traindata,\n",
    "    shuffle=False,\n",
    "    batch_size=train_batch_size)\n",
    "\n",
    "validdata=StoryDataset(encoded_dataset['valid'])\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    validdata,\n",
    "    shuffle=False,\n",
    "    batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd17c0-98c7-4d4c-b6a1-ba275e578687",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0742f-a6ef-4606-94a5-f178054e5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models\", #The output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    # load_best_model_at_end = True, \n",
    "    eval_steps = 40, # Number of update steps between two evaluations.\n",
    "    save_steps=50, # after # steps model is saved \n",
    "    warmup_steps=50# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['valid']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78062e3d-ebf9-4d33-8400-9becd0e2f23f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.to('cuda')\n",
    "model.eval()\n",
    "eval_loss=[]\n",
    "for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
    "    d1,d2,d3=inputs\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
    "        batch_loss=output[0]\n",
    "    eval_loss+=[batch_loss.cpu().item()]\n",
    "    del batch_loss\n",
    "eval_loss=np.mean(eval_loss)\n",
    "perplexity=math.exp(eval_loss)\n",
    "print(f'The average perplexity for valid dataset before fine-tuning is {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e17f9-d631-4aea-ae05-a2e0988bad2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f40b86-5c8f-49ee-959f-e78cff1b3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to('cuda')\n",
    "model.eval()\n",
    "eval_loss=[] \n",
    "for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
    "    d1,d2,d3=inputs\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
    "        batch_loss=output[0]\n",
    "    eval_loss+=[batch_loss.cpu().item()]\n",
    "    del batch_loss\n",
    "eval_loss=np.mean(eval_loss)\n",
    "perplexity=math.exp(eval_loss)\n",
    "print(f'The average perplexity for valid dataset after fine-tuning is {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109629e-bad0-4098-baa9-f6697b4f5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/1000_retoken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
