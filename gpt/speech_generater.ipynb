{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "added421-597c-487d-bcac-4ce4b419042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, GPT2Tokenizer, AutoTokenizer, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e945d33-d6fb-4f80-8f64-84da7addfcf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "custom_token = \"<sep>\"\n",
    "tokenizer.add_tokens([custom_token])\n",
    "\n",
    "# Assuming your folder structure is correct and the model checkpoint is at './../models/checkpoint-200/'\n",
    "folder_path = \"./\"\n",
    "model_name = \"model\"  # No trailing slash\n",
    "model_path = folder_path + model_name\n",
    "\n",
    "# Load the model from a local directory\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Resize the model embeddings to account for the new token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad0a1a0-e63e-4db1-b6b5-00a30c970119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanpunctuation(s):\n",
    "    for p in '!,.:;?':\n",
    "        s=s.replace(' '+p,p)\n",
    "    s=s.replace('-','')\n",
    "    s=s.replace('\\xa0',' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3364cb-e8c3-4910-a169-456bc526cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_text(prompt:str, max_len = 150, repetition_penalty = 2.0) -> str:\n",
    "    prompt = prompt +\": <sep>\"\n",
    "    input_ids = tokenizer.encode(prompt,\n",
    "                             return_tensors='pt')\n",
    "    \n",
    "    # set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "    seed = random.randrange(100000)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=max_len, \n",
    "        top_k=0,\n",
    "        temperature=0.7, \n",
    "        repetition_penalty=repetition_penalty, \n",
    "        num_return_sequences = 3\n",
    "    )\n",
    "    \n",
    "    # print(\"Output:\\n\"  + 100 * '-')\n",
    "    for idx, output in enumerate(sample_output):\n",
    "        print(\"\\n\\n=== GENERATED SEQUENCE {} ===\".format(idx + 1))\n",
    "        \n",
    "        decode_output = cleanpunctuation(tokenizer.decode(output, skip_special_tokens=True))\n",
    "        decode_output = \" \".join(decode_output.split(\"<sep>\")[0:2])\n",
    "        print(decode_output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbd4b53d-9b97-48d9-a8b6-99ea4d9e04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SEQUENCE 1 ===\n",
      "Ms. MURRAY:   Madam President on Sunday the Senate will vote to confirm Ulysses S Grant as secretary of Homeland Security for fiscal year 2016 and also proceed through a thorough review by our Armed Services Committee that is going forward with this nomination process so both parties can have an open debate about what are we trying every day in Congressto do right now? I want them ready when they get there but hopefully soon enough everyone at all who cares deeplyabout immigration should be able know exactly why it was done here last week because Amtrak fired its senior manager after he refused not only one questionbut just five questions from my staff regarding his actions during those eight hours alone which were unreported throughout yesterday morning news reports.... As far backas 2001 before\n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "Ms. MURRAY:   Madam President for 18 months I have been a supporter of the Keystone XL Pipeline and our work to prevent this pipeline from reaching its potential destination in North Dakota is well documented here on Capitol Hill by Congressman SANDERS who has authored dozens such legislation all over America including two identical bills that were introduced earlier today which would give UTRs private interests more control when it comes up with these \"propositions.\" There are many reasons why pipelines often fall short under constitutional standards both as American citizens but also because they include carbon emissions related taxes or other fees associated not onlyto meet their corporate obligationsbut through across State lines downriver where there may be some environmental impact if oil spills occurin addition we face an unexpected number\n",
      "=== GENERATED SEQUENCE 3 ===\n",
      "Ms. MURRAY:   Mr President and Ms Noble serve on the Subcommittee of Energy Sustainability for a numberof years now after working with Senator LEAHY to develop an energy policy that we believe would help America compete in lowcarbon sources such as natural gas or coal but also reduce global carbon pollution by reducing fossil fuel use over time through better regulation based upon industry standards rather than using outdated technology instead so our electricity generation can be more efficient while maintaining cultural traditions like those at Pearl Harbor who have spent much effort trying just how far they could go without burning too many gallons per day from their homeskilowatthours every year across these 1 million miles between Hawaiian average gallon is about 0 percent less compared todayto my knowledge this has ever\n"
     ]
    }
   ],
   "source": [
    "gene_text(\"Ms. MURRAY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7221cc-f513-4839-817c-032f3b61e557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
